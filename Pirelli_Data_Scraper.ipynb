{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3450896c",
   "metadata": {},
   "source": [
    "# Pirelli F1 Tire Data Web Scraper\n",
    "\n",
    "This notebook scrapes official tire and circuit data from Pirelli's website to complement F1 tire degradation analysis.\n",
    "\n",
    "## Target Data:\n",
    "- **Tire Compounds**: Soft/Medium/Hard compound specifications (C1-C5)\n",
    "- **Circuit Length**: Track distance in km\n",
    "- **Track Characteristics** (1-5 scale):\n",
    "  - Traction\n",
    "  - Asphalt Grip\n",
    "  - Tire Stress\n",
    "  - Braking\n",
    "  - Lateral Forces\n",
    "  - Downforce\n",
    "  - Asphalt Abrasion\n",
    "  - Track Evolution\n",
    "\n",
    "## Output:\n",
    "Structured DataFrame/CSV with all circuit data for analysis integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94cc57",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26e92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∑Ô∏è Pirelli Data Scraper Ready!\n",
      "üìä Target: Tire compounds and circuit characteristics (2022-2024)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üï∑Ô∏è Pirelli Data Scraper Ready!\")\n",
    "print(\"üìä Target: Tire compounds and circuit characteristics (2022-2024)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffefe5",
   "metadata": {},
   "source": [
    "## Scraper Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2fcef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Target years: [2022, 2023, 2024]\n",
      "‚è±Ô∏è Delay between requests: 1s\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://www.pirelli.com/global/en-ww/emotions-and-numbers/\"\n",
    "YEARS = [2022, 2023, 2024]\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Seconds - be respectful to the server\n",
    "\n",
    "# Headers to mimic a real browser\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "}\n",
    "\n",
    "print(f\"üéØ Target years: {YEARS}\")\n",
    "print(f\"‚è±Ô∏è Delay between requests: {DELAY_BETWEEN_REQUESTS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf8a36",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c94aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def get_page_content(url: str) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Safely fetch and parse a web page.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to fetch\n",
    "        \n",
    "    Returns:\n",
    "        BeautifulSoup object or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üì° Fetching: {url}\")\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)  # Be respectful\n",
    "        return soup\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_tire_compounds(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract tire compound information from the first infographic.\n",
    "    \n",
    "    Args:\n",
    "        soup: Parsed HTML content\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with tire compound info\n",
    "    \"\"\"\n",
    "    compounds = {'soft': None, 'medium': None, 'hard': None}\n",
    "    \n",
    "    try:\n",
    "        # Look for tire compound information\n",
    "        # This will need to be adjusted based on actual HTML structure\n",
    "        tire_elements = soup.find_all(text=re.compile(r'C[1-5]'))\n",
    "        \n",
    "        for element in tire_elements:\n",
    "            text = element.strip()\n",
    "            if 'C' in text:\n",
    "                compound_match = re.search(r'C([1-5])', text)\n",
    "                if compound_match:\n",
    "                    compound = f\"C{compound_match.group(1)}\"\n",
    "                    \n",
    "                    # Try to determine if it's soft, medium, or hard\n",
    "                    parent_text = element.parent.get_text().lower() if element.parent else text.lower()\n",
    "                    \n",
    "                    if 'soft' in parent_text or 'red' in parent_text:\n",
    "                        compounds['soft'] = compound\n",
    "                    elif 'medium' in parent_text or 'yellow' in parent_text:\n",
    "                        compounds['medium'] = compound\n",
    "                    elif 'hard' in parent_text or 'white' in parent_text:\n",
    "                        compounds['hard'] = compound\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting tire compounds: {e}\")\n",
    "    \n",
    "    return compounds\n",
    "\n",
    "\n",
    "def extract_circuit_length(soup: BeautifulSoup) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Extract circuit length from the page.\n",
    "    \n",
    "    Args:\n",
    "        soup: Parsed HTML content\n",
    "        \n",
    "    Returns:\n",
    "        Circuit length in km or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Look for circuit length patterns\n",
    "        length_patterns = [\n",
    "            r'(\\d+\\.\\d+)\\s*km',\n",
    "            r'(\\d+,\\d+)\\s*km',\n",
    "            r'length[^\\d]*(\\d+\\.\\d+)',\n",
    "            r'circuit[^\\d]*(\\d+\\.\\d+)'\n",
    "        ]\n",
    "        \n",
    "        page_text = soup.get_text()\n",
    "        \n",
    "        for pattern in length_patterns:\n",
    "            matches = re.findall(pattern, page_text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                # Convert comma to dot for European number format\n",
    "                length_str = matches[0].replace(',', '.')\n",
    "                return float(length_str)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting circuit length: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_track_characteristics(soup: BeautifulSoup) -> Dict[str, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Extract track characteristics (1-5 scale ratings).\n",
    "    \n",
    "    Args:\n",
    "        soup: Parsed HTML content\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with track characteristic ratings\n",
    "    \"\"\"\n",
    "    characteristics = {\n",
    "        'traction': None,\n",
    "        'asphalt_grip': None,\n",
    "        'tire_stress': None,\n",
    "        'braking': None,\n",
    "        'lateral': None,\n",
    "        'downforce': None,\n",
    "        'asphalt_abrasion': None,\n",
    "        'track_evolution': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Look for rating elements - this will need adjustment based on actual HTML\n",
    "        page_text = soup.get_text().lower()\n",
    "        \n",
    "        # Define search patterns for each characteristic\n",
    "        patterns = {\n",
    "            'traction': r'traction[^0-9]*([1-5])',\n",
    "            'asphalt_grip': r'(?:asphalt\\s*grip|grip)[^0-9]*([1-5])',\n",
    "            'tire_stress': r'(?:tire\\s*stress|tyre\\s*stress)[^0-9]*([1-5])',\n",
    "            'braking': r'braking[^0-9]*([1-5])',\n",
    "            'lateral': r'lateral[^0-9]*([1-5])',\n",
    "            'downforce': r'downforce[^0-9]*([1-5])',\n",
    "            'asphalt_abrasion': r'(?:asphalt\\s*abrasion|abrasion)[^0-9]*([1-5])',\n",
    "            'track_evolution': r'(?:track\\s*evolution|evolution)[^0-9]*([1-5])'\n",
    "        }\n",
    "        \n",
    "        for char_name, pattern in patterns.items():\n",
    "            matches = re.findall(pattern, page_text)\n",
    "            if matches:\n",
    "                characteristics[char_name] = int(matches[0])\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting track characteristics: {e}\")\n",
    "    \n",
    "    return characteristics\n",
    "\n",
    "\n",
    "def get_race_links_for_year(year: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get all race page links for a specific year.\n",
    "    \n",
    "    Args:\n",
    "        year: Year to scrape (2022-2024)\n",
    "        \n",
    "    Returns:\n",
    "        List of race page URLs\n",
    "    \"\"\"\n",
    "    year_url = f\"https://www.pirelli.com/global/en-ww/emotions-and-numbers/infographics-{year}/\"\n",
    "    soup = get_page_content(year_url)\n",
    "    \n",
    "    if not soup:\n",
    "        return []\n",
    "    \n",
    "    race_links = []\n",
    "    \n",
    "    try:\n",
    "        # Look for race links - this will need adjustment based on actual HTML structure\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            link_text = link.get_text().strip().lower()\n",
    "            \n",
    "            # Filter for race-related links\n",
    "            race_keywords = ['grand prix', 'gp', 'race', 'circuit', 'bahrain', 'saudi', 'australia', \n",
    "                           'imola', 'miami', 'spain', 'monaco', 'azerbaijan', 'canada', 'britain',\n",
    "                           'austria', 'france', 'hungary', 'belgium', 'netherlands', 'italy',\n",
    "                           'singapore', 'japan', 'qatar', 'usa', 'mexico', 'brazil', 'abu dhabi']\n",
    "            \n",
    "            if any(keyword in link_text for keyword in race_keywords):\n",
    "                full_url = urljoin(year_url, href)\n",
    "                if full_url not in race_links:\n",
    "                    race_links.append(full_url)\n",
    "        \n",
    "        print(f\"üèÅ Found {len(race_links)} race links for {year}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting race links for {year}: {e}\")\n",
    "    \n",
    "    return race_links\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ee311",
   "metadata": {},
   "source": [
    "## Main Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5e84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main scraping functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def scrape_race_data(race_url: str, year: int) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape all tire and circuit data from a single race page.\n",
    "    \n",
    "    Args:\n",
    "        race_url: URL of the race page\n",
    "        year: Year of the race\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with all extracted data or None if failed\n",
    "    \"\"\"\n",
    "    soup = get_page_content(race_url)\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extract race name from URL or page title\n",
    "        race_name = \"Unknown\"\n",
    "        title_element = soup.find('title')\n",
    "        if title_element:\n",
    "            title_text = title_element.get_text()\n",
    "            # Clean up the title to get race name\n",
    "            race_name = title_text.split('|')[0].strip() if '|' in title_text else title_text.strip()\n",
    "        \n",
    "        # Extract all data\n",
    "        compounds = extract_tire_compounds(soup)\n",
    "        circuit_length = extract_circuit_length(soup)\n",
    "        characteristics = extract_track_characteristics(soup)\n",
    "        \n",
    "        # Combine all data\n",
    "        race_data = {\n",
    "            'year': year,\n",
    "            'race_name': race_name,\n",
    "            'url': race_url,\n",
    "            'circuit_length_km': circuit_length,\n",
    "            'soft_compound': compounds['soft'],\n",
    "            'medium_compound': compounds['medium'],\n",
    "            'hard_compound': compounds['hard'],\n",
    "            **characteristics  # Unpack all track characteristics\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Extracted data for: {race_name} ({year})\")\n",
    "        return race_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping race data from {race_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_all_pirelli_data(years: List[int] = YEARS) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape tire and circuit data for all races across specified years.\n",
    "    \n",
    "    Args:\n",
    "        years: List of years to scrape\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all race data\n",
    "    \"\"\"\n",
    "    all_race_data = []\n",
    "    \n",
    "    print(f\"üöÄ Starting Pirelli data scraping for years: {years}\")\n",
    "    print(f\"üìä Target data: Tire compounds + 8 track characteristics\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"\\nüìÖ Processing year: {year}\")\n",
    "        \n",
    "        # Get all race links for this year\n",
    "        race_links = get_race_links_for_year(year)\n",
    "        \n",
    "        if not race_links:\n",
    "            print(f\"‚ö†Ô∏è No race links found for {year}\")\n",
    "            continue\n",
    "        \n",
    "        # Scrape each race\n",
    "        for i, race_url in enumerate(race_links, 1):\n",
    "            print(f\"\\nüèÅ Race {i}/{len(race_links)} for {year}\")\n",
    "            \n",
    "            race_data = scrape_race_data(race_url, year)\n",
    "            if race_data:\n",
    "                all_race_data.append(race_data)\n",
    "            \n",
    "            # Extra delay between races\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"üéâ Scraping complete! Collected data for {len(all_race_data)} races\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_race_data)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"üìä DataFrame shape: {df.shape}\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"‚úÖ Main scraping functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75801fd0",
   "metadata": {},
   "source": [
    "## Test Single Page First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54344201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing scraper on 2024 infographics page...\n",
      "üì° Fetching: https://www.pirelli.com/global/en-ww/emotions-and-numbers/infographics-2024/\n",
      "‚úÖ Successfully loaded test page\n",
      "üìÑ Page title: Emotions and Numbers: infographics 2024 | Pirelli\n",
      "üìù Sample content: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Emotions and Numbers: infographics 2024 | Pirelli\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IT\n",
      "EN\n",
      "ES\n",
      "BR\n",
      "DE\n",
      "FR\n",
      "‰∏≠ÂõΩ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Road\n",
      "\n",
      "\n",
      "Racing Spot\n",
      "\n",
      "\n",
      "Life\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Road overview\n",
      "Car\n",
      "Motorcycles\n",
      "Bicycles\n",
      "\n",
      "\n",
      "Racing Spot overview\n",
      "Formula 1\n",
      "Rally\n",
      "Gran Turismo\n",
      "Superbike\n",
      "Sailing\n",
      "Cycling\n",
      "Other Competitions\n",
      "E-sport\n",
      "\n",
      "\n",
      "Life overview\n",
      "Sustainability\n",
      "People\n",
      "Pirelli Calendar\n",
      "Lifestyle\n",
      "Innovation\n",
      "\n",
      "back\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Products\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Products\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Car Tyres\n",
      "\n",
      "\n",
      "Moto Tyres\n",
      "\n",
      "...\n",
      "üîó Found 256 links on the page\n",
      "   Link 1:  -> //www.pirelli.com/global/en-ww/homepage/\n",
      "   Link 2:  -> //www.pirelli.com/global/en-ww/facebook-newsletter\n",
      "   Link 3: IT -> //www.pirelli.com/global/it-it/homepage/\n",
      "   Link 4: EN -> //www.pirelli.com/global/en-ww/homepage/\n",
      "   Link 5: ES -> //www.pirelli.com/global/es-es/homepage/\n",
      "‚úÖ Successfully loaded test page\n",
      "üìÑ Page title: Emotions and Numbers: infographics 2024 | Pirelli\n",
      "üìù Sample content: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Emotions and Numbers: infographics 2024 | Pirelli\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IT\n",
      "EN\n",
      "ES\n",
      "BR\n",
      "DE\n",
      "FR\n",
      "‰∏≠ÂõΩ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Road\n",
      "\n",
      "\n",
      "Racing Spot\n",
      "\n",
      "\n",
      "Life\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Road overview\n",
      "Car\n",
      "Motorcycles\n",
      "Bicycles\n",
      "\n",
      "\n",
      "Racing Spot overview\n",
      "Formula 1\n",
      "Rally\n",
      "Gran Turismo\n",
      "Superbike\n",
      "Sailing\n",
      "Cycling\n",
      "Other Competitions\n",
      "E-sport\n",
      "\n",
      "\n",
      "Life overview\n",
      "Sustainability\n",
      "People\n",
      "Pirelli Calendar\n",
      "Lifestyle\n",
      "Innovation\n",
      "\n",
      "back\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Products\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Products\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Car Tyres\n",
      "\n",
      "\n",
      "Moto Tyres\n",
      "\n",
      "...\n",
      "üîó Found 256 links on the page\n",
      "   Link 1:  -> //www.pirelli.com/global/en-ww/homepage/\n",
      "   Link 2:  -> //www.pirelli.com/global/en-ww/facebook-newsletter\n",
      "   Link 3: IT -> //www.pirelli.com/global/it-it/homepage/\n",
      "   Link 4: EN -> //www.pirelli.com/global/en-ww/homepage/\n",
      "   Link 5: ES -> //www.pirelli.com/global/es-es/homepage/\n"
     ]
    }
   ],
   "source": [
    "# Test the scraper on a single page first to verify it works\n",
    "print(\"üß™ Testing scraper on 2024 infographics page...\")\n",
    "\n",
    "test_url = \"https://www.pirelli.com/global/en-ww/emotions-and-numbers/infographics-2024/\"\n",
    "test_soup = get_page_content(test_url)\n",
    "\n",
    "if test_soup:\n",
    "    print(\"‚úÖ Successfully loaded test page\")\n",
    "    print(f\"üìÑ Page title: {test_soup.find('title').get_text() if test_soup.find('title') else 'No title found'}\")\n",
    "    \n",
    "    # Show a sample of the page content\n",
    "    page_text = test_soup.get_text()[:500]\n",
    "    print(f\"üìù Sample content: {page_text}...\")\n",
    "    \n",
    "    # Test link extraction\n",
    "    links = test_soup.find_all('a', href=True)\n",
    "    print(f\"üîó Found {len(links)} links on the page\")\n",
    "    \n",
    "    # Show some sample links\n",
    "    for i, link in enumerate(links[:5]):\n",
    "        print(f\"   Link {i+1}: {link.get_text().strip()[:50]} -> {link['href'][:50]}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Failed to load test page - check URL and network connection\")\n",
    "    print(\"üí° You may need to adjust the scraping approach based on the site structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c35510",
   "metadata": {},
   "source": [
    "## Run Full Scraper\n",
    "\n",
    "**‚ö†Ô∏è Important Notes:**\n",
    "- This will make many requests to Pirelli's website\n",
    "- The scraper includes delays to be respectful\n",
    "- You may need to adjust the extraction functions based on the actual HTML structure\n",
    "- Run the test cell above first to verify the scraper works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full scraper (uncomment when ready)\n",
    "# pirelli_data = scrape_all_pirelli_data()\n",
    "\n",
    "# For now, let's start with just one year to test\n",
    "print(\"üéØ Starting with 2024 data only for testing...\")\n",
    "pirelli_data = scrape_all_pirelli_data([2024])\n",
    "\n",
    "# Display results\n",
    "if not pirelli_data.empty:\n",
    "    print(\"\\nüìä Scraped Data Summary:\")\n",
    "    print(pirelli_data.head())\n",
    "    \n",
    "    print(\"\\nüìà Data Info:\")\n",
    "    print(pirelli_data.info())\n",
    "    \n",
    "    print(\"\\nüîç Sample tire compounds:\")\n",
    "    compound_cols = ['soft_compound', 'medium_compound', 'hard_compound']\n",
    "    print(pirelli_data[compound_cols].head())\n",
    "    \n",
    "    print(\"\\nüèÅ Sample track characteristics:\")\n",
    "    char_cols = ['traction', 'asphalt_grip', 'tire_stress', 'braking']\n",
    "    available_char_cols = [col for col in char_cols if col in pirelli_data.columns]\n",
    "    if available_char_cols:\n",
    "        print(pirelli_data[available_char_cols].head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data scraped - check the extraction functions\")\n",
    "    print(\"üí° The HTML structure may be different than expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8434c",
   "metadata": {},
   "source": [
    "## Save Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to CSV for use in your tire degradation analysis\n",
    "if not pirelli_data.empty:\n",
    "    output_file = \"pirelli_tire_circuit_data.csv\"\n",
    "    pirelli_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"üíæ Data saved to: {output_file}\")\n",
    "    print(f\"üìä Shape: {pirelli_data.shape}\")\n",
    "    print(f\"üìã Columns: {list(pirelli_data.columns)}\")\n",
    "    \n",
    "    # Show data quality summary\n",
    "    print(\"\\nüîç Data Quality Summary:\")\n",
    "    missing_data = pirelli_data.isnull().sum()\n",
    "    print(missing_data[missing_data > 0])\n",
    "    \n",
    "    # Show unique tire compounds found\n",
    "    print(\"\\nüõû Tire Compounds Found:\")\n",
    "    for compound_type in ['soft_compound', 'medium_compound', 'hard_compound']:\n",
    "        if compound_type in pirelli_data.columns:\n",
    "            unique_compounds = pirelli_data[compound_type].dropna().unique()\n",
    "            print(f\"  {compound_type}: {unique_compounds}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a2975",
   "metadata": {},
   "source": [
    "## Integration with F1 Analysis\n",
    "\n",
    "Once you have the Pirelli data, you can merge it with your FastF1 analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eecb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to integrate with your F1 tire degradation analysis\n",
    "if not pirelli_data.empty:\n",
    "    print(\"üîó Integration Example:\")\n",
    "    print(\"\"\"# In your F1 analysis notebook:\n",
    "    \n",
    "import pandas as pd\n",
    "import fastf1\n",
    "\n",
    "# Load Pirelli data\n",
    "pirelli_data = pd.read_csv('pirelli_tire_circuit_data.csv')\n",
    "\n",
    "# Load F1 session\n",
    "session = fastf1.get_session(2024, \"Bahrain\", \"R\")\n",
    "session.load()\n",
    "\n",
    "# Process with your functions\n",
    "processed_laps = process_race_for_tire_analysis(session)\n",
    "\n",
    "# Merge with circuit characteristics\n",
    "race_name = \"Bahrain Grand Prix\"  # Match with pirelli_data\n",
    "circuit_data = pirelli_data[pirelli_data['race_name'].str.contains('Bahrain')]\n",
    "\n",
    "if not circuit_data.empty:\n",
    "    # Add circuit characteristics to your lap data\n",
    "    for col in ['traction', 'asphalt_grip', 'tire_stress', 'braking']:\n",
    "        if col in circuit_data.columns:\n",
    "            processed_laps[f'circuit_{col}'] = circuit_data[col].iloc[0]\n",
    "\n",
    "# Now you have lap-by-lap data with official circuit characteristics!\n",
    "# Perfect for advanced tire degradation modeling\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nüéØ Benefits:\")\n",
    "    print(\"‚úÖ Official tire compound data (C1-C5)\")\n",
    "    print(\"‚úÖ Circuit characteristics for modeling\")\n",
    "    print(\"‚úÖ Track evolution and abrasion data\")\n",
    "    print(\"‚úÖ Braking and lateral force intensity\")\n",
    "    print(\"‚úÖ Perfect complement to your FastF1 analysis!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No data available for integration example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6341bc",
   "metadata": {},
   "source": [
    "## Troubleshooting and Refinement\n",
    "\n",
    "If the scraper doesn't work perfectly on the first try, here are debugging tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97284b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug a specific race page\n",
    "def debug_race_page(race_url: str):\n",
    "    \"\"\"\n",
    "    Debug what's available on a specific race page.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Debugging: {race_url}\")\n",
    "    \n",
    "    soup = get_page_content(race_url)\n",
    "    if not soup:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüìÑ Page title:\")\n",
    "    title = soup.find('title')\n",
    "    print(title.get_text() if title else \"No title found\")\n",
    "    \n",
    "    print(\"\\nüî§ Text content (first 1000 chars):\")\n",
    "    print(soup.get_text()[:1000])\n",
    "    \n",
    "    print(\"\\nüñºÔ∏è Images found:\")\n",
    "    images = soup.find_all('img')\n",
    "    for i, img in enumerate(images[:5]):\n",
    "        alt_text = img.get('alt', 'No alt text')\n",
    "        src = img.get('src', 'No src')\n",
    "        print(f\"  Image {i+1}: {alt_text} -> {src[:50]}...\")\n",
    "    \n",
    "    print(\"\\nüî¢ Numbers found (potential ratings):\")\n",
    "    numbers = re.findall(r'\\b[1-5]\\b', soup.get_text())\n",
    "    print(f\"Found {len(numbers)} single digits 1-5: {numbers[:20]}...\")\n",
    "    \n",
    "    print(\"\\nüèéÔ∏è Tire-related text:\")\n",
    "    tire_text = re.findall(r'\\w*[Cc]\\d\\w*|\\w*tire\\w*|\\w*compound\\w*', soup.get_text())\n",
    "    print(f\"Found: {tire_text[:10]}...\")\n",
    "\n",
    "# Uncomment to debug a specific page\n",
    "# debug_race_page(\"https://www.pirelli.com/global/en-ww/emotions-and-numbers/infographics-2024/\")\n",
    "\n",
    "print(\"üõ†Ô∏è Debug function ready - uncomment the line above to use it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93762d94",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive web scraper for Pirelli's F1 tire and circuit data.\n",
    "\n",
    "### What it scrapes:\n",
    "1. **Tire Compounds** - Official C1-C5 designations for soft/medium/hard\n",
    "2. **Circuit Length** - Track distance in kilometers\n",
    "3. **Track Characteristics** (1-5 scale):\n",
    "   - Traction\n",
    "   - Asphalt Grip\n",
    "   - Tire Stress\n",
    "   - Braking\n",
    "   - Lateral Forces\n",
    "   - Downforce\n",
    "   - Asphalt Abrasion\n",
    "   - Track Evolution\n",
    "\n",
    "### Next Steps:\n",
    "1. **Test** the scraper on a few pages first\n",
    "2. **Refine** extraction functions based on actual HTML structure\n",
    "3. **Run** full scraper for all years (2022-2024)\n",
    "4. **Integrate** data with your F1 tire degradation analysis\n",
    "\n",
    "### Integration with your analysis:\n",
    "The scraped data perfectly complements your FastF1 analysis by providing:\n",
    "- Official tire compound context\n",
    "- Circuit-specific characteristics for modeling\n",
    "- Track evolution and surface data\n",
    "- Braking and cornering intensity metrics\n",
    "\n",
    "This will make your tire degradation analysis much more comprehensive! üèéÔ∏èüìä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
